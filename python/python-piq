#!/usr/bin/env python


# TODO
#
# - more restrictive parsing -- allow transform only inside [ ... ] or single ()


import sys
import StringIO

import tokenize
import token as pytoken
import keyword as pykeyword


def is_token(l, i, toknum, tokval=None):
    if i >= len(l):
        return False

    token = l[i]
    if toknum == token[0]:
        if tokval == None:
            return True
        else:
            return (tokval == token[1])
    else:
        return False


def is_token_op(l, i, opname=None):
    return is_token(l, i, pytoken.OP, tokval=opname)


def is_token_op_in(l, i, opnames):
    for opname in opnames:
        if is_token_op(l, i, opname):
            return True
    return False


def is_token_name(l, i):
    return is_token(l, i, pytoken.NAME)


def peek_token(l, i):
    if i >= len(l):
        return None
    else:
        return l[i]


def pop_token(l, i):
    return (l[i], i + 1)


def is_token_keyword(l, i):
    token = peek_token(l, i)
    return (token is not None and token[0] == pytoken.NAME and pykeyword.iskeyword(token[1]))


def is_token_op_value_start(l, i):
    return is_token_op_in(l, i, ['`', '(', '[', '{', '-', '+', '~'])


def is_token_value_start(l, i):
    return (not is_token_op(l, i) or is_token_op_value_start(l, i)) and not is_token_keyword(l, i) and not is_token(l, i, pytoken.ENDMARKER)


def make_token(toknum, tokval, tokstart = None, tokend = None):
    if tokstart is None:
        return (toknum, tokval)
    else:
        if tokend is None:
            tokend = tokstart
        return (toknum, tokval, tokstart, tokend, None)


def is_piq_name_start(l, i):
    return is_token_op(l, i, '.') and is_token_name(l, i + 1)


def is_piq_name_continue(l, i):
    return is_token_op(l, i, '-') and is_token_name(l, i + 1)


# skip insignificant tokens
def skip_nl_and_comment_tokens(l, i, accu):
    if is_token(l, i, tokenize.NL) or is_token(l, i, tokenize.COMMENT):
        token, i = pop_token(l, i)
        accu.append(token)
        return skip_nl_and_comment_tokens(l, i, accu)  # see if we've got more of these
    else:
        return i


def transform_piq_name(filename, l, i, accu, name=None, name_loc=None):
    # '.' in case of name start, '-' in case of another name segment
    #
    # TODO: make sure '-' immediately follow preceeding name segment
    dot_or_dash_token, i = pop_token(l, i)
    dot_loc = dot_or_dash_token[3]
    dot_or_dash = dot_or_dash_token[1]

    name_token, i = pop_token(l, i)
    name_token_val = name_token[1]

    if name is None:
        name = ''
    else:
        name += dot_or_dash
    name += name_token_val

    if name_loc is None:
        name_loc = dot_loc

    def accu_append_keyword(value_loc=None):
        def accu_append_loc(loc):
            accu.extend([
                (pytoken.OP, ','),
                (pytoken.OP, '('),
                (pytoken.STRING, "'" + filename + "'"),
                (pytoken.OP, ','),
                (pytoken.NUMBER, str(loc[0])),  # line
                (pytoken.OP, ','),
                (pytoken.NUMBER, str(loc[1])),  # column
                (pytoken.OP, ')')
            ])

        accu.extend([
            (pytoken.NAME, 'piq.Keyword'),
            (pytoken.OP, '('),
            (pytoken.STRING, "'" + name + "'")
        ])

        accu_append_loc(name_loc)

        if value_loc is not None:
            accu_append_loc(value_loc)

        accu.extend([
            (pytoken.OP, ')')
        ])

    if is_piq_name_start(l, i):
        # next token is also a name => this name is chained with another Piq
        # name => recurse
        i = transform_piq_name(filename, l, i, accu, name, name_loc)
    elif is_piq_name_continue(l, i):
        # next token is a '-' followed by another name segment => recurse
        i = transform_piq_name(filename, l, i, accu, name, name_loc)
    else:
        # something else

        # skip whitespace
        nl_and_comment_accu = []
        i = skip_nl_and_comment_tokens(l, i, nl_and_comment_accu)

        if is_token_op_in(l, i, [')', ']', ',']):
            # end of name
            accu_append_keyword()
        elif is_token_value_start(l, i):
            # value juxtaposition
            value_token = peek_token(l, i)
            value_token_loc = value_token[3]
            accu_append_keyword(value_token_loc)

            accu.append((pytoken.OP, '**'))
        elif is_token_op(l, i, '*') and is_token_value_start(l, i + 1):
            # splice
            value_token = peek_token(l, i + 1)
            value_token_loc = value_token[3]
            accu_append_keyword(value_token_loc)
        else:
            # something else, likely an error
            #
            # TODO, XXX: generate an error right away instead of letting Python
            # do it -- Python errors look ugly with expanded keywords
            accu_append_keyword()

        accu.extend(nl_and_comment_accu)

    return i


def transform_tokens(filename, l):
    accu = []
    i = 0

    i = skip_nl_and_comment_tokens(l, i, accu)
    accu.extend([
        (tokenize.NL, '\n'),
        (pytoken.NAME, 'import'),
        (pytoken.NAME, 'piq'),
        (tokenize.NL, '\n')
    ])

    piq_name_allowed = False
    while True:
        if i >= len(l):
            return accu

        if piq_name_allowed and is_piq_name_start(l, i):
            i = transform_piq_name(filename, l, i, accu)

            # Piq name can not be immediately followed by another Piq name
            piq_name_allowed = False
        else:
            # Piq name is allowed only after these tokens
            #
            # TODO: allow only commas inside lists
            piq_name_allowed = is_token_op_in(l, i, ['(', '[', ','])

            token, i = pop_token(l, i)
            accu.append(token)

            i = skip_nl_and_comment_tokens(l, i, accu)


def transform(filename, infile):
    in_tokens = tokenize.generate_tokens(infile)
    return transform_tokens(filename, list(in_tokens))


def transform_string(s):
    filename = '-'
    return transform(filename, StringIO.StringIO(s).readline)


def transform_file(filename):
    with open(filename, 'r') as infile:
        return transform(filename, infile.readline)


def main():
    arg_from_string = False
    arg_transform = False
    arg_tokenize = False

    args = sys.argv[1:]

    i = 0
    while True:
        if i >= len(args):
            break

        a = args[i]

        if a in ['-t', '--transform']:
            arg_transform = True
        elif a == '--from-string':
            arg_from_string = True
        elif a == '--tokenize':
            arg_tokenize = True
        elif a.startswith('-'):
            pass
        else:
            break  # positional argument
        i += 1

    positional_arg = args[i]

    if arg_from_string:
        input_string = positional_arg
        filename = '<string>'
        out_tokens = transform_string(input_string)
    else:
        filename = positional_arg
        out_tokens = transform_file(filename)

    if arg_tokenize:
        res = []
        for token in out_tokens:
            res.append((pytoken.tok_name[token[0]], token[1]))
        print res
    else:
        source = tokenize.untokenize(out_tokens)
        if arg_transform:
            print source
        else:
            exec(compile(source, filename, 'exec'), globals())


if __name__ == '__main__':
    main()
